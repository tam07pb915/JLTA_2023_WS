---
title: "Rを用いた一般化線形混合モデル（GLMM）の分析手法を身につける:言語研究分野の事例をもとに"
subtitle: "日本言語テスト学会（JLTA）第26回（2023年度）全国研究大会ワークショップ"
author: "田村祐（関西大学）"
date: "2023-09-10"
output:
  slidy_presentation:
    css: pulse.css
---

---

# おしながき
* はじめに
    + 経緯
    + 注意事項
* WSの準備
    + RおよびRStudioの準備
* 理論編：一般（化）線形（混合）モデルの概略
* 実践編：データ分析の実際
  - データの下処理
  - データ分析
  - 分析結果の解釈
  - 結果の図示と報告
* おわりに

# はじめに  


- 今回のWSの分析の部分は下記の研究論文（データ，RコードすべてOSFにてオンライン公開済）のものです。
- 二値データの分析：Terai, M., Fukuta, J., & Tamura, Y. (2023). Learnability of L2 collocations and L1 influence on L2 collocational representations of Japanese learners of English. _International Review of Applied Linguistics in Language Teaching_. https://doi.org/10.1515/iral-2022-0234

※反応時間の分析も扱おうと思っていたのですが，ロジスティック回帰に加えて反応時間のGLMMをゼロから扱うのは3時間では足りないと判断して断念しました。田村のOSFページに反応時間の分析を扱った下記の論文のRコードがありますので，反応時間の分析に興味がお有りの方はそちらをご参照ください。

- Tamura, Y. (2023). Investigation of the relationship between animacy and L2 learners’ acquisition of the English plural morpheme. _Journal of Psycholinguistic Research_, 52, 675–690. https://doi.org/10.1007/s10936-022-09915-2
- Tamura, Y. (2023). Is _cats_ one word or two? L2 Learners’ processing of number marking in English from the viewpoints of form–meaning mapping. _Second Language Research_. https://doi.org/10.1177/02676583231188933  

# 注意事項①

* 本WSでは，Rの基本的な動作トラブルにつきましては個別に対応できない場合がございます
* Rの基本的な操作等に習熟されたい方につきましては，書籍等御覧ください。  
 - [『Rによる教育データ分析入門』](https://www.ohmsha.co.jp/book/9784274225918/)
 - [教育・心理系研究のためのＲによるデータ分析―論文作成への理論と実践集](https://amzn.asia/d/a1GD3R3)
 - 最近はChatGPTもRについての質問に割りと答えてくれます（間違えたりもしますが）

# 注意事項②

* 本WSは3時間の設定となっていますが，実際のGLMMの[習得]{.text-danger}には，その[倍以上]{.text-danger}の復習が必要となります
* 今回使用するコードやデータはすべてweb上で公開していますので，復習の際にご利用ください
  - GitHub: [https://github.com/tam07pb915/JLTA_2023_WS/](https://github.com/tam07pb915/JLTA_2023_WS/)
* 個別の分析に関するご相談にはお応えできない可能性が高いですので，参考文献をご参照ください

# 注意事項③

* 今回は，時間の都合上により線形混合効果モデル（Linear Mixed-Effect Model）は扱いません
* ただし，LMEの発展がGLMMであり，LMEは確率分布の指定が不要なだけですのでR上の分析はほぼ同じで，Rの関数で言えば下記のようになります
  * `lme4::glmer()`-> GLMM
  * `lme4::lmer()` -> LME
* 評定値のデータの分析について
  - 事前のアンケートで評定値データの扱いを書かれている方がいましたがすみません，今回は扱いません
  - リッカート尺度の分析方法
    - 間隔尺度とみなして線形モデルで分析する（`lme4::lmer()`使う）
    - 順序尺度とみなして順序ロジットモデルで分析（`ordinal::clmm()`を使う）
    - 順序ロジットについては次の論文が参考になります：小林雄一郎. (2022). 学習者コーパス研究におけるマルチレベル順序ロジットモデルの活用. https://doi.org/10.31219/osf.io/tkqym


# 簡単に自己紹介

## 自身のこと
- 名古屋大学大学院国際開発研究科博士課程修了（2018年3月修了）
- 2018年4月より関西大学外国語学部に着任
- 専門は第二言語の文法習得や文処理
  - Tamura, Y. (2023). Is cats one word or two? L2 Learners’ processing of number marking in English from the viewpoints of form–meaning mapping. _Second Language Research_. https://doi.org/10.1177/02676583231188933
  - Tamura, Y., Fukuta, J., Nishimura, Y., & Kato, D. (2022). Rule-based or efficiency-driven processing of expletive there in English as a foreign language. _International Review of Applied Linguistics in Language Teaching_. https://doi.org/10.1515/iral-2021-0156
- 趣味はサッカー観戦でガンバ大阪を応援しています  

## データ分析について
* 2014年(D1年時)に俗に言う「緑本」[（久保, 2012）]{.smaller}を読んでから統計モデリングを勉強するように
* Nagoya.R #12という勉強会で，「[一般化線形混合モデル入門の入門](https://www.slideshare.net/yutamura1/ss-42303827)」というRを使った線形混合効果モデルのやり方について発表
* その後から自分の研究でもLMEやGLMMを使うように  
  - [Tamura, Y. (2015). Reinvestigating consciousness-raising grammar task and noticing. _JABAET Journal, 19_, 19–47. -> mixed-effect logistic regression]{.smaller}
  - [Tamura, Y., Fukuta, J., Nishimura, Y., Harada, Y., Hara, K., & Kato, D. (2019). Japanese EFL learners’ sentence processing of conceptual plurality: An analysis focusing on reciprocal verbs. _Applied Psycholinguistics, 41_, 59–91. -> Gamma GLMM]{.smaller}  
* 気づいたらこのこのネタでのトークを頼まれるように
  - 田村祐 (2019). 「統計ワークショップ」JACET英語語彙・英語辞書・リーディング研究会合同研究会. 早稲田大学.[資料](https://github.com/tam07pb915/JACET-SIG_GLMM-Workshop)
  - 田村祐 (2021). 「一般化線形混合モデルの実践ー気をつけたい3つのポイント」連続公開講座「データサイエンス時代の英語教育」第2回 言語教育研究の実際. 主催：名古屋大学大学院人文学研究科 英語教育分野. オンライン開催. [投影資料](https://speakerdeck.com/tam07pb915/2021-11-06-lmm-and-glmm)
  - 田村祐（2022). 「Rによる混合効果モデル分析の勘所」外国語教育メディア学会（LET）関西支部 メソドロジー研究部会2021年度第3回研究会.
* 今回の発表資料の中には上記のワークショップ等で用いた資料と重複する部分もあります  


# 一般（化）線形（混合）モデルの概略
## 確率分布を考える
* 一般線形モデル
  - いわゆる普通の回帰分析（単回帰，重回帰）
  - [正規分布]{.text-danger}が仮定される
  - t検定や分散分析も線形モデルの一種
* 一般化線形モデル
  - [正規分布以外]{.text-danger}の確率分布族を用いるもの
    - 回数データ（単位時間あたりの生起回数）->ポアソン分布，負の二項分布
    - 二値データ（正答・誤答）-> 二項分布
    - 非負の連続量（反応時間等）->ガンマ分布，逆正規分布  

## なぜ一般"化"線形モデルなのか
* 世の中には正規分布しないデータもたくさんある
* 正規分布してなくてもデータ分析はできる
* データが生み出された[メカニズム]{.text-danger}に着目して分析できる
* 確率分布とリンク関数の組み合わせでより柔軟でデータの実体に即した分析が可能
* 導入としては，『緑本』がおすすめです  

久保拓哉(2012). 『データ解析のための統計モデリング入門：一般化線形モデル・階層ベイズモデル・MCMC』. 岩波書店  


## 混合効果（mixed effect）とはなにか
* [固定効果]{.text-danger}＋[変量効果]{.text-danger}のこと
* 分散分析でいうF1[（被験者分析）]{.smaller}とF2[（項目分析）]{.smaller}を同時にやってしまうようなもの
* 固定効果->説明変数（独立変数）のこと。いわゆる研究者が見たいもの
* 変量効果->応答変数（従属変数）に影響を与えてしまうばらつき
  - 追試を行うときに変わるモノ->変量効果
      * 例: 単語の頻度効果を調べる追試
          - 固定効果->単語の頻度（高・低またはあるコーパスでの頻度）
          - 変量効果->実験参加者，単語（もとの研究と同じ刺激語であっても）  

* 実験系でよく扱うデータは参加者と項目[（e.g,テスト項目や刺激文）]{.smaller}のばらつきを考慮しなくてはいけない
  - 例1：ある項目・参加者だけ難しい[（or 反応時間が長い）]{.smaller}->ランダム切片  
  - 例2：ある項目・参加者だけ頻度効果[（説明変数の影響）]{.smaller}が大きい->ランダム傾き  

## 応用言語学の分野でも新しくない
* Rと，混合効果モデルが簡単に実装可能な*lme4*パッケージの爆発的普及で一般化
* 混合効果モデルのイントロ論文として私がいつもおすすめするもの
  - Cunnings, I. (2012). An overview of mixed-effects statistical models for second language researchers. *Second Language Research, 28*, 369–382. [doi:10.1177/0267658312443651](https://doi.org/10.1177/0267658312443651)
  - Linck, J. A. & Cunnings, I. (2015), The utility and application of mixed‐effects models in second language research. *Language Learning, 65*, 185-207. [doi:10.1111/lang.12117](https://doi.org/10.1111/lang.12117)
  - Gries, S. T. (2021). (Generalized Linear) Mixed-Effects Modeling: A Learner Corpus Example. _Language Learning_, 71(3), 757–798. https://doi.org/10.1111/lang.12448
  - Meteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. _Journal of Memory and Language_, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092
  
# これだけは言いたい

> “[T]here is [no single correct way]{.text-danger} to implement an LMM, and…the choices they [researchers] make during analysis will comprise one path, however justified, amongst multiple alternatives. ” (Meteyard & Davies, 2020, pp.1–2)
  
# 混合効果モデルの数学的な背景（Barr et al., 2013とそれに基づいた山口, 2018より)
## 一般的な単回帰モデル
固定効果（独立変数）が一つのシンプルな単回帰モデルを考えてみます。

$$Y_{si} = \beta_0 + \beta_1 X_i + e_{si}, \quad e_{si} \sim N(0, \sigma^2)$$

* $Y_{si}$ : $_s$番目の参加者の$_i$番目の項目に対する反応（従属変数，または応答変数）
* $\beta_0$: 固定効果の切片
* $\beta_1$: 固定効果の傾き
* $X_i$: 独立変数（または説明変数）
* $e_{si}$: 平均が0，分散$\sigma^2$の正規分布($N$)に従う誤差 
* Rのコードだと下記のようになります。　　

`lm(Y ~ X, data=data)`  

## 混合効果モデルの回帰式
### 切片に変量効果を考慮したモデル

$$ Y_{si} = \beta_0 + \color{Red}{S_{0s}} + \beta_1 X_i + e_{si},\\
\color{Red}{\quad S_{0s} \sim N(0, \tau_{00}^2)}, \\
\quad e_{si} \sim N(0, \sigma^2)$$

* $Y_{si}$ : $_s$番目の参加者の$_i$番目の項目に対する反応（従属変数，または応答変数）
* $\beta_0$: 固定効果の切片
* $\beta_1$: 固定効果の傾き
* $X_i$: 独立変数（または説明変数）
* $e_{si}$: 平均が0，分散$\sigma^2$の正規分布($N$)に従う誤差
* $\color{Red}{S_{0s}}$: 参加者ごとに変動する切片（ランダム切片）で，平均が0，$\tau_{00}^2$の正規分布（$N$）に従う
* ラテンのシンボルである$\beta_0$，$\beta_1$, $\sigma^2$, $\tau_{00}^2$はすべてパラメータで，これを推定することになる
* Rのコードだと下記のようになります。  

`model <-lmer(Y ~ X + (1|subject), data=data)`  


### 参加者だけではなく項目の切片にも変量効果を考慮したモデル

$$
Y_{si} = \beta_0 + S_{0s} + \color{Red}{I_{0i}} + \beta_1 X_i + e_{si}, \\ 
\quad S_{0s} \sim N(0, \tau_{00}^2), \\ 
\color{Red}{\quad I_{0i} \sim N(0, \omega_{00}^2)}, \\
\quad e_{si} \sim N(0, \sigma^2)
$$

* $Y_{si}$ : $_s$番目の参加者の$_i$番目の項目に対する反応（従属変数，または応答変数）
* $\beta_0$: 固定効果の切片
* $S_{0s}$: 参加者ごとに変動する切片（ランダム切片）で，平均が0，$\tau_{00}^2$の正規分布（$N$）に従う
* $\color{Red}{I_{0i}}$: 項目ごとに変動する切片（ランダム切片）で，平均が0，$\omega_{00}^2$の正規分布（$N$）に従う
* $\beta_1$: 固定効果の傾き
* $X_i$: 独立変数（または説明変数）
* $e_{si}$: 平均が0，分散$\sigma^2$の正規分布($N$)に従う誤差
* Rのコードだと下記のようになります。    

`model <-lmer(Y ~ X + (1|subject)+(1|item), data=data)`  


### 切片だけではなく傾きの変量効果も考慮したモデル(まずは参加者の傾きだけ)

$$
Y_{si} = \beta_0 + S_{0s} + I_{0i} + \color{Red}{(\beta_1 + S_{1s}) X_i} + e_{si},\\
\color{Red}{(S_{0s},S_{1s})
\sim N 
\left( 
0,
\begin{pmatrix}
\tau_{00}^2 & \rho \tau_{00} \tau_{11}  \\
\rho \tau_{00} \tau_{11} & \tau_{11}^2
\end{pmatrix}
\right)},\\
I_{0i} \sim N(0, \omega_{00}^2),\\
\quad e_{si} \sim N(0, \sigma^2)
$$

* $Y_{si}$ : $_s$番目の参加者の$_i$番目の項目に対する反応（従属変数，または応答変数）
* $\beta_0$: 固定効果の切片
* $S_{0s}$: 参加者ごとに変動する切片（ランダム切片）で，平均が0，$\tau_{00}^2$の正規分布（$N$）に従う
* $I_{0i}$: 項目ごとに変動する切片（ランダム切片）で，平均が0，$\omega_{00}^2$の正規分布（$N$）に従う
* $\beta_1$: 固定効果の傾き
* $\color{Red}{S_{1s}}$: 参加者ごとに変動する傾き（ランダム傾き）で，平均が0，$\tau_{11}^2$の正規分布（$N$）に従う
* $X_i$: 独立変数（または説明変数）
* $e_{si}$: 平均が0，分散$\sigma^2$の正規分布($N$)に従う誤差
* 一般的に，[切片と傾きには相関がある]{.text-danger}ことが仮定されたモデルを作ります
* つまり，$S_{0s}$（切片）と$S_{1s}$（傾き）が独立した正規分布に従うのではなく，[一つの二変量正規分布]{.text-danger}から推定されるということになります
  * $\color{Red}{\tau_{00}^2}$: 参加者のランダム切片の分散
  * $\color{Red}{\tau_{11}^2}$: 参加者のランダム傾きの分散
  * $\color{Red}{\rho}$: 参加者のランダム切片とランダム傾きの間の相関係数
  * $\color{Red}{\rho \tau_{00} \tau_{11}}$: 参加者のランダム切片とランダム傾きの共分散  
  * Rのコードだと下記のようになります。  

`model <-lmer(Y ~ X + (1+X|subject)+(1|item), data=data)`    
  
### 項目の傾きも入れたモデル（最大モデル）


\[
Y_{si} = \beta_0 + S_{0s} + I_{0i} + (\beta_1 + S_{1s} + \color{red}{I_{1i}}) X_i + e_{si},
\]
\[
\begin{pmatrix}
S_{0s},S_{1s}
\end{pmatrix}
\sim N 
\left( 
0,\begin{pmatrix}
\tau_{00}^2 & \rho \tau_{00} \tau_{11} \\
\rho \tau_{00} \tau_{11} & \tau_{11}^2
\end{pmatrix}
\right),
\]
\[
\color{red}{
\begin{pmatrix}
I_{0i} ,I_{1i}
\end{pmatrix}
}
\sim N 
\left( 
0, 
\begin{pmatrix}
\omega_{00}^2 & \rho' \omega_{00} \omega_{11} \\
\rho' \omega_{00} \omega_{11} & \omega_{11}^2
\end{pmatrix}
\right),
\]
\[
e_{si} \sim N(0, \sigma^2)
\]

  
* $Y_{si}$ : $_s$番目の参加者の$_i$番目の項目に対する反応（従属変数，または応答変数）
* $\beta_0$: 固定効果の切片
* $S_{0s}$: 参加者ごとに変動する切片（ランダム切片）で，平均が0，$\tau_{00}^2$の正規分布（$N$）に従う
* $I_{0i}$: 項目ごとに変動する切片（ランダム切片）で，平均が0，$\omega_{00}^2$の正規分布（$N$）に従う
* $\beta_1$: 固定効果の傾き
* $S_{1s}$: 参加者ごとに変動する傾き（ランダム傾き）で，平均が0，$\tau_{11}^2$の正規分布（$N$）に従う
* $I_{0i}$: 項目ごとに変動する傾き（ランダム傾き）で，平均が0，$\omega_{11}^2$の正規分布（$N$）に従う
* $X_i$: 独立変数（または説明変数）
* $e_{si}$: 平均が0，分散$\sigma^2$の正規分布($N$)に従う誤差
* 参加者のランダム効果（$S_{0s}$と$S_{1s}$）だけでなく，項目のランダム効果（$I_{0i}$と$I_{1i}$)についても同様の分散共分散行列があることがわかります
  * $\color{Red}{\omega_{00}^2}$: 項目のランダム切片の分散
  * $\color{Red}{\omega_{11}^2}$: 項目のランダム傾きの分散
  * $\color{Red}{\rho}$: 項目のランダム切片とランダム傾きの間の相関係数
  * $\color{Red}{\rho \omega_{00} \omega_{11}}$: 項目のランダム切片とランダム傾きの共分散
  * Rのコードだと下記のようになります。  

`model <-lmer(Y ~ X + (1+X|subject)+(1+X|item), data=data)`  
  
## いくつかのポイント
* 統制のとれた項目（問題アイテム，刺激文）であれば，基本的には項目のランダム効果は参加者のそれより小さくなる傾向があります
* [項目のランダム傾きをモデルに入れるかどうかは，実験デザインに依存]{.text-danger}します（※なんでもかんでもランダム効果に入れればいいわけではない）
  * ランダム傾きを推定する固定効果が一つの項目に対して複数条件ある場合は項目のランダム傾きを考慮する意味がある（例：文法性判断課題における正文・非文の影響）
  * 同じ項目で複数条件ないような場合は項目のランダム傾きを考慮する意味がない（例：文法性判断課題における文の長さの影響）
  * もちろん，文法性判断課題における非文と正文で文の長さが変わる（語数が変わる）ような場合はランダム傾きを考慮する意味が出てきます
* ランダム効果の推定において，ランダム傾きとランダム切片の間の相関を仮定する最も複雑なモデルは，推定がうまくいかないこともあります
  * 複数のランダム傾きがモデルに含まれる場合，推定するパラメータの数がかなり多くなるため
  * そもそも傾きと切片にほとんど相関が見られない場合
* その場合，ランダム切片と傾きの間の相関パラメータを抜いて，切片と傾きがそれぞれ独立した正規分布に従うという推定をします（あとでR上でやります）
  
# 今回のWSで扱うロジスティック回帰分析の概要(田村, 2019,とほぼ同じ説明です)
* 0/1（成功・失敗）のデータを扱うときに使う
* 一般的な「テスト」のデータも，1問が正解・不正解という2値のデータの蓄積
* 良くない例
  - 15点満点のデータを13点，10点，9点...のように扱う（欠損があったらどうする？）
  - 比率データに置き換える（連続変数ですか？もとはカテゴリカルデータですよね？）

## 二項分布を用いる
  - $n$回（ただし $n>0$ ）の試行のうちの成功回数 $k$ が発生する確率 $q$ を求める
  - 各試行は独立な試行（ベルヌーイ試行）

$$p(k|N,q)=_{N}C_{k}*q^{k}(1-q)^{N-k}$$

* コインを10回投げて4回表が出る確率は？[（ただしq=0.5とする）]{.smaller}  
$p(4|10,0.5)=_{10}C_{4}0.5^{4}(1-0.5)^{10-4}=$ `r round(210*0.5^4*((1-0.5)^(10-4)),3)`

### ロジスティック関数の導入

*例：30人の学習者がいて，それぞれの学習者がある文法問題に正解する確率を $q_{i}$としたときのロジスティック回帰モデル（ $z_{i}$には線形予測子が入っていると考える）

$$q_{i}={\rm logistic} (z_{i})=\frac{1}{1+\exp(-z_{i})}$$

## ロジスティック関数からロジット関数へ
* 先ほどのロジスティック関数を変形すると次のようになる

$${\rm log}(\frac{q_{i}}{1-q_{i}})=z_{i}$$

* この式の左辺はロジット関数と呼ばれる
* ロジスティック関数とロジット関数は逆関数の関係にある
* ロジスティクス関数と線形予測子の関係をつなぐリンク関数がロジットリンク関数

## ロジスティック回帰のまとめ
* 下記のような方程式を解くこと

$${\rm log}(\frac{q_{i}}{1-q_{i}})=\beta_0 + \beta_1x_{ij}...$$

* 個人ごとの成功確率である $q_{i}$は得られたデータからわかることであり， $X_{1}$は独立変数の値である
* $\beta_{0}$と $\beta_{1}...$を最尤推定[(maximum Likelihood estimation)]{.smaller}で求める
* オッズ（ $\frac{q_{i}}{1-q_{i}}$）の対数($log$)が線形予測子と等しいので次のようにも表現できる

$$\frac{q_{i}}{1-q_{i}}=\exp(\beta_{0}+\beta_{1}X_{1}...)$$

$$=\exp(\beta_{0})*\exp(\beta_{1}X_{1})...$$

# 実践編
## 分析するデータはどんなデータか
- Terai et al. (2023)は日本語を第一言語とする英語学習者を対象に，コロケーションの容認性判断課題を実施
- 実験では，コンピュータ画面上に提示される2語のコロケーションが英語として自然であるかの判断が求められた
- その後，参加者はコロケーションを英語から日本語に翻訳する課題を行い，その際の翻訳の難しさを4段階で評価することを求められた
- 翻訳の難しさ評定値は第一言語の影響（L1 influence）の代理指標
- コロケーションには次の3つの種類とベースライン条件項目（英語でも日本語でも不自然なコロケーション）があった
  - 一致性のあるコロケーション（JE collocations）：thick fog/厚い霧, bitter experience/苦い経験
  - 日本語にのみ存在するコロケーション（J-only collocations）：?sweet judgement/甘い判断，熱い戦い/hot battle
  - 英語にのみ存在するコロケーション（E-only collocations）：busy road/?忙しい道/混雑した道，lucky guess/?運のいい推測/まぐれあたりの推測
  - ベースライン条件項目：ランダムな形容詞と名詞と形容詞の組み合わせ(e.g., severe learner, proud idea)
  
##  研究課題
1. 翻訳の難しさ評定値（L1 influence）が容認性判断課題の正答率に与える影響は熟達度によって変わるのか
2. コロケーションのタイプによって，熟達度の影響は異なるのか

[というのが実際の論文なのですが，今回は実際の論文で行われたものと少し異なる分析をします（実際の分析では名義尺度の変数をつかっていませんが，名義尺度の変数の分析も扱いたいため）。]{.text-danger}

# 準備
## RStudioの起動
* ファイルのダウンロード
 1. Clone or download->Download ZIPでファイルをすべてDLし，解凍
 2. 解凍したZIPファイルの中のWS.Rprojを起動
 3. slide.Rmdの中にあるRコードを走らせていく

## すすめ方
* ここからは，実際にRを動かしながらデータ分析を行っていきます
* これより先，灰色の背景で囲まれた部分は全てRのコードを示しているとご理解ください
* GitHubから.Rmdファイルまたは.htmlファイルをDLしてコピペすることをおすすめします

# データ分析の下準備
## パッケージのインストール
下記のコード[（#以降はコメントアウトですので入力しなくてもOK）]{.smaller}を実行してパッケージのインストールをお願いします。

```{r,eval=F, include=T, echo=T}
install.packages("lme4") #分析
install.packages("sjPlot") #可視化
install.packages("emmeans") #下位検定と可視化
install.packages("tidyverse") #ハンドリング
install.packages("psych") #記述統計
install.packages("performance") #信頼性係数等
install.packages("irr") #評定者間信頼性
install.packages("gplots") #可視化
install.packages("car") #モデルの評価
```

## パッケージの読み込み
インストールしたパッケージを使えるようにします。

```{r,warning=F,message=F,echo=T}
library(lme4)
library(sjPlot)
library(emmeans)
library(tidyverse)
library(psych)
library(performance)
library(irr)
library(gplots)
library(car)
```


## データの読み込み
Terai et al. (2023)のデータを読み込みます。必ず`AJ.csv`のファイルが今作業しているフォルダの中にあることを確認してください。

```{r}
AJ <-read.csv("AJ.csv", header = T)[,-1] #[,-1]がついているのは，1列目の行番号が不要だからです
```

## データの確認
```{r}
head(AJ)
```

* subject: 参加者
* itemID: 項目番号
* pres.order: 提示順序
* Item: 提示された2語の刺激
* Itemtype: コロケーションのタイプ（JE, Jonly, Eonly, Baseline)
* Length: 2語の刺激の長さ（文字数）
* ColFreq: コロケーションの頻度
* AdjFreq: コロケーションの1語目の形容詞の頻度
* NounFreq: コロケーションの2語目の名詞の頻度
* MIScore: 相互情報量（MI）スコア（2つの語がどれだけ共起しやすいか）
* answer: 模範解答
* res: 参加者の反応（0: 誤答，1: 正答）
* RT: 反応時間

## 課題の信頼性係数（Cronbach's alpha）をチェック
まずは課題の信頼性をチェックします。ただ，データの型がその分析のために合っていませんよね？回帰モデルのためには縦型（long型）のデータ形式である必要がありますが，項目分析や信頼性係数の算出のためには行が項目，列が参加者の行列になっている必要があります。
```{r,warning=FALSE}
AJ %>%
  select(subject, itemID, res) %>% #被験者ID，項目ID，参加者の0/1の反応だけを選んでね
  pivot_wider(names_from = itemID, values_from = res) %>% #itemIDを列番号にして，セルに入る数値はresから持ってきてね
  {as.data.frame(.[,-1])} %>% #データ・フレーム形式にして，1列目にあるsubject列は信頼性係数分析のためには不必要なので抜いてね
  psych::alpha(.,n.iter=2000) #ここまで処理したデータをpsychパッケージのalpha()関数に渡してね
```

なんだか色々出てきているけれども，raw_alphaのところを確認でとりあえずはOK。信頼区間が3つ（Feldt, Duhacheck, bootstrapped）算出されているが，計算の仕方が違う。ヘルプを見ると，次のようにあるので，bootstrap法で算出したと報告すればOK？＞識者

>Because both of these procedures use normal theory, if you really care about confidence intervals, using the boot option (n.iter > 1) is recommended.

別の関数でもCronbach's alphaを算出。

```{r}
AJ %>% 
  select(subject, itemID,res) %>% 
  pivot_wider(.,names_from = itemID,values_from = res) %>% 
    {as.data.frame(.[,-1])} %>% 
  performance::cronbachs_alpha()
```

ほとんど変わらないので問題なさそうですね。

## 頻度情報をlog変換

頻度情報はlog変換して分析される（差分を小さくするため）ので，頻度情報の入った列のデータをログ変換して別の列に保存します。ログ変換の際に，0があるとやっかいなので1を足しておきます。

```{r}
AJ$ColFreq<-AJ$ColFreq+1
AJ$MIScore<-AJ$MIScore+1
```

```{r}
AJ%>%
  mutate(across(c(ColFreq, AdjFreq, NounFreq, MIScore),log,.names = "{.col}_log"))->AJ
head(AJ) 
```

データフレームの右端に，`ColFreq_log`など，頻度情報の列名に`_log`がついた列が追加されているのがわかりますね。ちなみに，ここで使っている`mutate()`関数と`across()`関数の組み合わせはとても便利です。詳しくは下記の拙ブログ記事をご参照ください。　　

[[R] mutateとacrossでデータの下処理を少しだけエレガントに](https://tam07pb915.com/2021/08/22/r-mutate-and-across/)　　

## 翻訳課題のデータ
### 翻訳課題のデータの読み込み
翻訳課題のデータは別のcsvファイルに保存されているので，分析に使えるように，容認性判断課題のデータと結合します。まずは，翻訳課題を`Trans`という変数に入れます。

```{r}
Trans<-read.csv("Translation.csv", header = T); head(Trans)
```

* subject: 参加者ID
* itemID: 項目ID
* order: 提示順序
* condition: コロケーションのタイプ（1: JE, 2: Eonly, 3: Jonly）
* word: 提示された刺激
* JP: 学習者の翻訳（空欄は翻訳できなかった場合）
* rating: 翻訳の難しさ
* RaterA: 評価者Aの評価
* RaterB: 評価者Bの評価
* Agreement: AとBの一致度
* Final: 正しく翻訳ができていると言えるかどうかの最終的な判断

### 翻訳課題の評価者間一致度
```{r}
Trans %>%
  select(Agreement) %>% #Agreement列を選択
  table()->agree_tab #データを表形式で提示したものをagree_tabに入れる
print(agree_tab) #agree_tabの中身を表示
```
一致していないのが77件，一致しているのが981件というのがわかりましたので，これを単純に割合にすればいいですね。

```{r}
agree_tab[2]/(agree_tab[1]+agree_tab[2])*100
```
`r round(981/(77+981)*100,3)`%の一致度でした。

続いて，評定者間信頼性係数のCohen's Kappaを出してみます。

```{r}
cbind(Trans$RaterA,Trans$RaterB) %>% kappa2
```

## 熟達度テスト（Oxford Quick Placement Test; OQPT）のデータ
次に，熟達度テストのデータを準備します。え？まだ準備なの？と思われるかもしれませんが，実際問題このあたりは論文で言うとメインのデータ分析の説明の前にくる部分です。ただし，こういうのも全部Rでやっておくことで，データ分析の透明性が格段にあがります。「RのことはいいからGLMMのことを早く教えてほしいんだけど...？」と思ったそこのあなた！こういうことをおろそかにしてはいけませんよ！

### OQPTデータの読み込み

```{r}
oqpt <-read.csv("OQPT_results.csv",header=T,sep=",")
head(oqpt)
```

### OQPTの記述統計
```{r}
apply(oqpt[,-1],1,sum)->oqpt_sum #パイプ演算子（ %>% ）を使ってもいいのですが，あとで合計得点のデータを熟達度の代理指標として容認性判断課題のデータに読み込むのであえて変数に入れています
describe(oqpt_sum)
```

平均が37.88 ( _SD_ = 6.38 )となっています。

### OQPTの信頼性係数

熟達度テストの信頼性係数を報告せよ！はよく言われますよね。というわけでこちらも信頼性係数を出しておきましょう。

```{r, cache=T}
psych::alpha(oqpt[,-1], n.iter = 2000)
```
あまり高くはないですが，まあそこまで悪いというわけでもなさそうですね。

## データの結合
### OQPTデータをz変換
テストスコアをそのまま使うのではなく正規化してから使うのでその処理をしておきます
```{r}
oqpt_sum%>%
  as.data.frame()%>% #いったんデータフレームに変換
  mutate(across(1,~scale(.x)[,1],.names = "z.oqpt"))->oqpt_sum #scale()関数の処理をして，それを"z.oqpt"という名前にする
```

### 翻訳課題データとOQPTデータの結合
Transというデータフレームには1人につき69行のデータがあるので，1人のOQPTスコアを69回ずつ入れてあげることになります。

```{r}
nrow(Trans)
Trans$oqpt <-rep(oqpt_sum[,1],each=69)  #Transという変数にoqptという列を作り，opqt_sumの1列目からデータを取ってきてそれぞれの値を69回繰り返す
Trans$z.oqpt <-rep(oqpt_sum[,2],each=69) #z変換した数値も同様に。
str(Trans) #oqptとz.oqptという列ができているのがわかります
```

### 被験者の除外
実は，ID001とID016の参加者は，容認性判断課題時にプログラムの不具合が起こってデータが全て消えてしまったので，この2名のデータを除外します。

```{r}
Trans2 <- dplyr::filter(Trans,subject != 1 & subject != 16) #filter()関数で除外して，Trans2という新しい変数に。こういう処理をしたときは，新しい変数に入れることをおすすめします
```

## ベースライン条件のデータを除外して翻訳課題データと容認性判断課題データを結合
ベースライン条件のデータは，翻訳のデータがありません。したがって，ベースライン条件のデータを除外してから結合します。


```{r}
dplyr::left_join(AJ,Trans2,by = c('subject','itemID'))->dat #subjectとitemID列を使って2つのデータを照合して結合
str(dat)
```

上のデータをよく見ると，翻訳課題のwordという列と，容認性判断課題のItemという列が重複していることがわかります。また，Itemtypeとconditionも同じ情報ですので余剰ですね[（こういうのは実験プログラムを作る際に起こさないようにしないといけないですね...）]{.smaller}。よって，これらの重複を除外します。

```{r}
select(dat,-c(word, condition))->dat1 #datからwordとcondition列を抜いてdat1に保存
str(dat1)
```
さて，これでデータの準備ができました！

# データ分析（記述統計と可視化）
## 翻訳課題の記述統計

```{r}
dat1 %>% 
  filter(Itemtype == "JE") %>% 
  select(res,Final) %>% 
  xtabs(~+res+Final,.)%>%
  balloonplot(.,xlab = "Acceptability",ylab = "Translation",main = "")
```

上図からわかるように， 容認性判断課題と翻訳課題の両方に正解したのが全体の84.6％で，容認性判断で不正解となった88回答のうち、70.5％が翻訳タスクで正解していることがわかります。それでは，Eonlyの項目についても見てみます。

```{r}
dat1 %>% 
  filter(Itemtype == "Eonly") %>% 
  select(res,Final) %>%
  xtabs(~+res+Final,.)%>%
  balloonplot(.,xlab = "Acceptability",ylab = "Translation",main = "")
```

Eonlyでは傾向が全く異なりますね。容認性判断課題で不正解となった項目のおよそ3割ほどが翻訳課題では正答になっています。また，容認性判断課題における正答の4割弱が翻訳課題では誤答と判断されています。翻訳が誤っていたものと，翻訳に無回答のデータは正しい知識を保有していないと判断し，このあとの分析から除外します。ただし，JonlyとBaselineは翻訳の正答・誤答がないので，分けて処理します。

4つが`Itemtype`という列に入っていますが，JonlyとBaselineは"NO"が正解となる項目である一方で，JEとEonlyは"YES"が正解になる項目で，一概に比較ができません。したがって，分けて分析を行ってみます。

### データの分割と誤訳の排除
```{r}
dat1 %>% 
  dplyr::filter(Itemtype %in% c("JE", "Eonly")) %>% 
  dplyr::filter(Final==1)->dat_y

dat1 %>% 
  dplyr::filter(Itemtype %in% c("Jonly", "Baseline"))->dat_n
```


## 容認性判断課題の記述統計
### JE
```{r}
dat_y%>%
  dplyr::filter(Itemtype == "JE") %>% 
  dplyr::group_by(subject)%>%
  summarize(Accuracy = mean(res,na.rm=T))%>%
  print(n=Inf)%>% #print all data
  summarize(.,describe(Accuracy)) #descriptive stats
```

### Eonly
```{r}
dat_y%>%
  dplyr::filter(Itemtype == "Eonly") %>% 
  dplyr::group_by(subject)%>%
  summarize(Accuracy = mean(res,na.rm=T))%>%
  print(n=Inf)%>% #print all data
  summarize(.,describe(Accuracy)) #descriptive stats
```

### Jonly
```{r}
dat_n%>%
  dplyr::filter(Itemtype == "Jonly") %>% 
  dplyr::group_by(subject)%>%
  summarize(Accuracy = mean(res,na.rm=T))%>%
  print(n=Inf)%>% #print all data
  summarize(.,describe(Accuracy)) #descriptive stats
```

### Baseline
```{r}
dat_n%>%
  dplyr::filter(Itemtype == "Baseline") %>% 
  dplyr::group_by(subject)%>%
  summarize(Accuracy = mean(res,na.rm=T))%>%
  print(n=Inf)%>% #print all data
  summarize(.,describe(Accuracy)) #descriptive stats
```

## 容認性判断課題の正答率の可視化

グラフにするときは4つ横並びにしたいので，バラしたデータフレームをくっつけるというやや冗長なことをしています。


```{r, message=F}
combined_data <- bind_rows(
  dat_y %>% dplyr::select(subject, Itemtype, res),
  dat_n %>% dplyr::select(subject, Itemtype, res)
)
combined_data %>%
  dplyr::group_by(subject) %>%
  dplyr::group_by(Itemtype, .add = TRUE) %>%
  summarize(Accuracy = mean(res, na.rm = TRUE)) %>%
  ggplot(aes(x = Itemtype, y = Accuracy)) +
  geom_jitter(aes(color = Itemtype), alpha = 0.5,
              width = 0.15, height = 0,
              show.legend = FALSE) +
  geom_boxplot(aes(fill = Itemtype),
               alpha = 0.5, show.legend = FALSE) +
  ylim(0, 1) +
  labs(x = "Item Type", y = "Accuracy") +
  scale_x_discrete(
    limit = c("JE", "Eonly", "Jonly", "Baseline"),
    labels = c("Congruent", "L2-only", "L1-only", "Baseline")
  ) +
  theme(
    axis.text.x = element_text(size = 15),
    axis.text.y = element_text(size = 15),
    axis.title.x = element_text(size = 20),
    axis.title.y = element_text(size = 20)
  )
```

# 混合効果ロジスティック回帰モデル
## 下準備
### 頻度データの正規化

ログ変換した頻度列を，正規化します。

```{r}
str(dat_y)
str(dat_n)
```

全体でscaleすると，平均と分散が小さくなってしまうので，参加者ごとに取り出して処理して，元の形に戻します。
```{r}
dat_y %>%
  group_by(subject) %>%
  mutate(across(contains("log"), ~scale(.x)[,1],.names = "z.{col}")) %>%
  ungroup() -> dat_y

dat_n %>%
  group_by(subject) %>%
  mutate(across(contains("log"), ~scale(.x)[,1],.names = "z.{col}")) %>%
  ungroup() -> dat_n
```

### 評定値データの正規化

4段階の評定値データを正規化します。
```{r}
dat_y %>% 
  mutate(across(rating,~scale(.x)[,1],.names = "z.{.col}"))->dat_y

dat_n %>% 
  mutate(across(rating,~scale(.x)[,1],.names = "z.{.col}"))->dat_n
```

### カテゴリカル変数のコーディング

カテゴリカル変数（名義尺度のデータ）をそのまま分析に使うと，ダミー・コーディング（トリートメント・コーディング）になります。これは2つ以上ある水準の1つをベースラインとし，そのベースラインと他の水準を比較するというものです。もしも，1変数のみであれば特に問題ありませんが，2変数以上の場合にダミーコーディングを使っていると，ある変数の主効果が別の変数にネストしてしまうことになります。　　
例えば，2×2のデザインを考えます。変数Aと変数Bがあるとき，変数Aの主効果は，変数Bがダミーコーディングであれば変数BがゼロのときのAの「主効果」になります。これは，実際には「単純主効果」と呼ばれるものであり，分散分析の「主効果」ではありません。よって，結果の解釈には注意が必要になります。このような場合には，全体平均を切片とする解釈が可能になるサム・コーディング(シンプル・コーディング）が必要です。これは，2水準であればある水準を-0.5,もう一方の水準を0.5とするようなコーディング方法です。2つ（以上）のカテゴリカル変数の交互作用が含まれない場合には，この心配は必要ありません。コーディング方法については，Schad et al. (2020)が非常に詳しいのでおすすめです。

### コーディング

contrasts()関数を使う方法もありますが，個人の経験で，変量効果のパラメータ推定時に相関外す手続きがうまくできないので，数字のコーディングをして新しく列を作る方法をおすすめします。

```{r}
ifelse(dat_y$Itemtype == "Eonly", -0.5, 0.5) -> dat_y$Itemtype_c #cはcontrastの略のつもりです。Eonlyが-0.5でJEが0.5
ifelse(dat_n$Itemtype == "Baseline", -0.5, 0.5) -> dat_n$Itemtype_c #cはcontrastの略のつもりですBaselineが-0.5でJEが0.5
```

# YESデータセットの分析

GLMMは分析を関数を何回も回してモデルを何個も作ります。よって，最初にリスト形式の「箱」を用意して，そこにモデルを入れいくようにするといいです。

```{r}
m_y<-list()
```

まずは，興味のある変数（翻訳の評価，熟達度，コロケーション・タイプ）以外の共変量である頻度データのうち，どれがモデルを説明するのかを吟味して，今回の容認性判断課題の反応に影響のあるものだけ選び取ります。このような方法を前向き（forward）の方法と呼びます。変数が多くて，最も複雑なモデルから単純化するのが困難な場合に採用されます（このくらいの変数じゃ多いと言わないかもですが）。

##  切片のみモデル
```{r, cache=T}

m_y[[1]] <-glmer(res ~ (1|subject)+(1|itemID),
               data = dat_y, #データの指定
               family=binomial, #binomialにするとロジスティック回帰になります
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))) #これはおまじないだと思ってください
```

##  コロケーションの頻度を入れたモデル
```{r, cache=T}

m_y[[2]] <-glmer(res ~ z.ColFreq_log+(1|subject)+(1|itemID),
               data = dat_y,
               family=binomial,
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5)))
```

##  形容詞の頻度を入れたモデル
```{r, cache=T}

m_y[[3]] <-glmer(res ~ z.AdjFreq_log+(1|subject)+(1|itemID),
               data = dat_y,
               family=binomial,
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5)))
```

## 名詞の頻度を入れたモデル
```{r, cache=T}

m_y[[4]] <-glmer(res ~ z.NounFreq_log+(1|subject)+(1|itemID),
               data = dat_y,
               family=binomial,
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5)))
```

## MI Scoreを入れたモデル
```{r, cache=T}

m_y[[5]] <-glmer(res ~ z.MIScore_log+(1|subject)+(1|itemID),
               data = dat_y,
               family=binomial,
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5)))
```

5つのモデルをAICで比較します。
```{r}
sapply(m_y,AIC)%>%data.frame
sapply(m_y,AIC)%>%which.min
```

5番目のモデル（MI Scoreを入れたモデル）が最もAICが低いですね。というわけで，MI Scoreを入れたモデルにさらに別の変数を入れて比較していきます。

## さらに追加する変数を選択
```{r}
m_y2<-list()
```

## MI Scoreを入れたモデルをベースに

```{r}
m_y2[[1]]<-m_y[[5]]
```

## 新たに変数を追加
先程と同じようにすべての式を書いてもいいのですが，面倒なので，`update()`関数を使います。
```{r, cache=T}
m_y2[[2]]<-update(m_y2[[1]],.~.+z.ColFreq_log) #.~.は「元の式」の意味。そこにz.ColFreq_logを足してね，ということ。変数を抜くときはマイナス
m_y2[[3]]<-update(m_y2[[1]],.~.+z.AdjFreq_log)
m_y2[[4]]<-update(m_y2[[1]],.~.+z.NounFreq_log)

```

```{r}
sapply(m_y2,AIC)%>%data.frame
sapply(m_y2,AIC)%>%which.min
```
コロケーション頻度を入れた2番目のモデルが最もAICが低いですね。ただ，数字の差がそこまで大きくありません。尤度比検定してみましょう。

```{r}
anova(m_y2[[1]],m_y2[[2]])
```
Mtuschek et al. (2017)では，α ~LRT~ = .10 or .20の基準で有意ならより複雑なモデルを選択することが推奨されているので，コロケーション頻度も入れましょう。

## さらに追加する変数を選択
```{r}
m_y3<-list()
```

## コロケーション頻度も追加されたモデルをベースに
```{r}
m_y3[[1]]<-m_y2[[2]]
```

## 新たに変数を追加
```{r, cache=T}
m_y3[[2]]<-update(m_y3[[1]],.~.+z.AdjFreq_log)
m_y3[[3]]<-update(m_y2[[1]],.~.+z.NounFreq_log)

```

AICを比較。

```{r}
sapply(m_y3,AIC)%>%data.frame
sapply(m_y3,AIC)%>%which.min
```

形容詞の頻度や名詞の頻度を足してもモデルが向上しないようです。よって，これらの変数はいれません。共変量を入れたモデルは次のようになりました。

```{r}
summary(m_y3[[1]])
```

# YESデータ分析の本番はここから
それでは，ここで興味のある3つの変数の分析に入りましょう。

```{r}
m_y4<-list()
```

下記のコードでは，`glmer()`関数を，`system.time()`という関数に渡しています。これは，計算にどれくらいの時間がかかるかを可視化するためです。今回はそこまで時間がかかりませんが，データセットの大きさやモデルの複雑さによっては，かなりの時間がかかる場合もあります。コードを共有する際に，データを再分析する側の人が，どれくらいの時間がかかるのか，コードを実行する前にわかるように，処理時間が表示されるようにしています。結果の単位は「秒」です。

```{r, cache=T}
system.time(m_y4[[1]] <-glmer(res ~ z.MIScore_log+z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt*Itemtype_c|subject)+(1+z.oqpt+z.oqpt:Itemtype_c|itemID),
             data = dat_y,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```

試しに今熟達度とコロケーションタイプの交互作用を入れたモデルを作ってみましたが，警告が出ました。この警告は，推定しようとしている分散のパラメータが0に近いものがあると出てきます。モデルが複雑すぎるので，ランダム効果を削る事を考えます。この際に，主成分分析を行い，分散が0に近いものがあれば問題があります。

```{r}
rePCA(m_y4[[1]]) %>% summary
```

```{r, cache=T}
system.time(m_y4[[2]] <-glmer(res ~ z.MIScore_log+z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt*Itemtype_c||subject)+(1+z.oqpt+z.oqpt:Itemtype_c||itemID),
             data = dat_y,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
相関パラメータを外してもだめなようです。では，また主成分分析の結果を見てみましょう。

```{r}
rePCA(m_y4[[2]]) %>% summary
```
項目，参加者，両方の変量効果に分散がゼロのものが含まれています。では，モデルの中身を見てみましょう。

```{r}
summary(m_y4[[2]])
```

```{r, cache=T}
system.time(m_y4[[3]] <-glmer(res ~ z.MIScore_log+z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt+Itemtype_c||subject)+(1+z.oqpt+z.oqpt:Itemtype_c||itemID),
             data = dat_y,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```


```{r}
rePCA(m_y4[[3]]) %>% summary
```

```{r}
summary(m_y4[[3]])
```
参加者のItemtype_cのランダム傾きが非常に小さいのがわかります。これを抜いて新しくモデルを作ります。

```{r, cache=T}
system.time(m_y4[[4]] <-glmer(res ~ z.MIScore_log+z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt||subject)+(1+z.oqpt:Itemtype_c||itemID),
             data = dat_y,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```

まだ収束しません。

```{r}
rePCA(m_y4[[4]]) %>% summary
```

まだ項目のランダム傾きで分散が小さいものがあるようです。

```{r}
summary(m_y4[[4]])
```

交互作用項をなくしてみましょう。

```{r}
system.time(m_y4[[5]] <-glmer(res ~ z.MIScore_log+z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt||subject)+(1+z.oqpt||itemID),
             data = dat_y,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```

残念ながら収束しません。

```{r}
summary(m_y4[[5]])
```
項目ごとの熟達度テストスコアの傾きの分散がゼロなので，それを抜きます。このとき，`(1||itemID)`としないように注意します。傾きがないので，相関パラメータもありませんからね。

```{r}
system.time(m_y4[[6]] <-glmer(res ~ z.MIScore_log+z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt||subject)+(1|itemID),
             data = dat_y,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
収束したようです。では，参加者のランダム傾きの相関パラメータを戻してみましょう。

```{r}
system.time(m_y4[[7]] <-glmer(res ~ z.MIScore_log+z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt|subject)+(1|itemID),
             data = dat_y,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
収束しました。尤度比検定してみます。

```{r}
anova(m_y4[[7]],m_y4[[6]])
```
パラメータが増えても残念ながらモデルは良くなっていません。よって，相関パラメータのないモデルを採用します。


## 最終的なモデル
```{r}
summary(m_y4[[6]])
```

## モデルの評価

回帰モデルは，説明変数間の相関が高いと多重共線性の問題があります。その評価のために，VIF(Variance Inflation Factor)をチェックします。このVIFの値がいくつなら問題になるのかは諸説ありますが，田中他 (2008, p.116）は，VIFが5より大きければ多重共線性の可能性が大きく注意が必要と述べています。

```{r}
vif(m_y4[[6]])
```


## 交互作用の図示
残念ながら交互作用は非有意ですが，そのことを図示して確かめましょう。

```{r}
plot_model(m_y4[[6]],type = "int")
```

## 結果の解釈
- まず，コロケーションの頻度とMIスコアの頻度は係数が有意です。よって
  - コロケーションの頻度が高くなると容認性判断課題の正答率が高くなる
  - MIスコアが高くなると容認性判断課題の正答率が高くなる
- 次に，熟達度テストスコアの係数が有意ですから，熟達度があがれば，容認性判断課題の正答率もあがるといえます
- さらに，コロケーションタイプの係数も有意ですから，JE項目のほうが，Eonly項目よりも容認性判断課題の正答率が高いといえます

# オッズ比の算出
## 効果量（オッズ比）を求める①
* ある事象の起こる確率が $p$で表されるとき，オッズは次のようになる

$$Odds = \frac{p}{1-p}$$

* オッズ比はオッズの比で表されるので，確率 $p$と確率 $q$のオッズ比は次のようになる

$$\frac{\frac{p}{1-p}}{\frac{q}{1-q}}$$


## 効果量（オッズ比）を求める②
* ロジット関数を思い出すと，これはオッズの対数であり，商の対数は対数の差と等しいので次のように表せる

$${\rm log}(\frac{q_{i}}{1-q_{i}})={\rm log}(q_{i})-log(1-q_{i})$$

* つまり，2つの確率のロジットの差がオッズ比の対数である

$${\rm log}(\frac{p}{1-p})-{\rm log}(\frac{q}{1-q})={\rm log}(\frac{\frac{p}{1-p}}{\frac{q}{1-q}})$$

## 効果量（オッズ比）を求める③
* このオッズ比の仕組みが今理解できなくても大丈夫です
* 大事なことは次のこと

## [偏回帰係数を指数変換するとオッズ比]{.text-danger}

* Rではexp()という関数があるので，オッズ比は次のように求められる
```{r}
fixef(m_y4[[6]]) %>% #fixefは固定効果の係数を取り出す関数
  exp()
```

例えば，z.oqptのオッズ比は`r round(fixef(m_y4[[6]])[4] %>% exp(),3) `ですから，z.oqptが1単位増加すると（今回は正規化しているので，熟達度テストの1単位あたりの変動は1SDの変動になります），容認性判断課題に正答する確率が1.589倍あがると解釈できます。

# NOデータ分析
## ちょっとした注意事項

最初に，翻訳データと熟達度テストデータを結合し，その後に容認性判断課題のデータとくっつけました。したがって，今，Baselineアイテムの行には熟達度テストデータが入っていません。また，Baselineアイテムは翻訳課題に含まれていないので，翻訳の難しさの評定値データもありません。つまり，こちらの分析にはz.ratingの変数は使えません。また，どうにかして熟達度テストデータをBaselineアイテムの行に持ってこないといけません。

`oqpt_sum`データに被験者番号を付与して，その番号を使って元のデータと紐づけます。

```{r}
head(oqpt_sum)

oqpt_sum$subject<-1:nrow(oqpt)
names(oqpt_sum)[1]<-"oqpt"
```

結合には，`left_join()`関数を使います。

```{r}
left_join(oqpt_sum,dat_n, by="subject") %>% 
  dplyr::select(-contains(".y")) %>% 
  dplyr::filter(.,subject != 1 & subject != 16)->dat_n2
```

.xという識別子がつくので，それを削除します。

```{r}
names(dat_n2) <- gsub("\\.x$", "", names(dat_n2))
```

`Baseline`項目にも`oqpt`や`z.oqpt`のスコアの列が入っていることを確認しましょう。

```{r}
dat_n2 %>% 
  filter(Itemtype == "Baseline") %>% 
  head()
```

では，YESデータセットの分析と同様に，まずは共変量としてどの変数を入れるかを検討します。　　

```{r}
m_n<-list()
```

##  切片のみモデル

```{r}
m_n[[1]] <-glmer(res ~ (1|subject)+(1|itemID),
               data = dat_n2, #データの指定
               family=binomial, #binomialにするとロジスティック回帰になります
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))) #これはおまじないだと思ってください
```

####  コロケーションの頻度を入れたモデル
```{r}

m_n[[2]] <-glmer(res ~ z.ColFreq_log+(1|subject)+(1|itemID),
               data = dat_n2,
               family=binomial,
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5)))
```

####  形容詞の頻度を入れたモデル
```{r}

m_n[[3]] <-glmer(res ~ z.AdjFreq_log+(1|subject)+(1|itemID),
               data = dat_n2,
               family=binomial,
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5)))
```

#### 名詞の頻度を入れたモデル
```{r}

m_n[[4]] <-glmer(res ~ z.NounFreq_log+(1|subject)+(1|itemID),
               data = dat_n2,
               family=binomial,
               glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5)))
```

#### MI Scoreを入れたモデル
MI ScoreはBaselineとJonlyは計算できないので，ここでは考慮しません。

4つのモデルをAICで比較します。
```{r}
sapply(m_n,AIC)%>%data.frame
sapply(m_n,AIC)%>%which.min
```

2番目のコロケーションの頻度を入れたものがAICが一番低いので，それをベースにしてYESデータセットの分析と同じ手順を繰り返します。

## さらに追加する変数を選択
```{r}
m_n2<-list()
```

## コロケーション頻度を入れたモデルをベースに

```{r}
m_n2[[1]]<-m_n[[2]]
```

## 新たに変数を追加
先程と同じようにすべての式を書いてもいいのですが，面倒なので，`update()`関数を使います。
```{r}
m_n2[[2]]<-update(m_n2[[1]],.~.+z.AdjFreq_log)
m_n2[[3]]<-update(m_n2[[1]],.~.+z.NounFreq_log)

```

```{r}
sapply(m_n2,AIC)%>%data.frame
sapply(m_n2,AIC)%>%which.min
```

名詞の頻度や形容詞の頻度を入れてもモデルは向上していないようです。ということで，NOデータセットの分析にはコロケーション頻度だけを共変量にします。

```{r}
summary(m_n2[[1]])
```

# NOデータ分析の本番はここから
それでは，ここで興味のある3つの変数の分析に入りましょう。

```{r}
m_n3<-list()
```

```{r}
system.time(m_n3[[1]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt*Itemtype_c|subject)+(1+z.oqpt+z.oqpt:Itemtype_c|itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
`?isSingular`のエラーが出ています。主成分分析の結果を見てみます。

```{r}
rePCA(m_n3[[1]]) %>% summary
```

分散がゼロの要素があります。まずは相関パラメータを外してみましょう。

```{r}
system.time(m_n3[[2]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt*Itemtype_c||subject)+(1+z.oqpt+z.oqpt:Itemtype_c||itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
まだ同じエラーが出ます。

モデルの中身を見て，どれを除外するか，見当をつけましょう。

```{r}
summary(m_n3[[2]])
```
項目の交互作用の傾きがゼロなので，それを抜きます。項目タイプのランダム傾きはモデルに入らないデザインなので抜きます。

```{r}
system.time(m_n3[[3]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt*Itemtype_c||subject)+(1+z.oqpt||itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```

まだダメです。

```{r}
rePCA(m_n3[[3]]) %>% summary
```
```{r}
summary(m_n3[[3]])
```

項目の`z.oqpt`の傾きを除きます。

```{r}
system.time(m_n3[[4]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_c+(1+z.oqpt+Itemtype_c||subject)+(1|itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```

まだ問題があります。

```{r}
rePCA(m_n3[[4]]) %>% summary
summary(m_n3[[4]])
```

参加者の`z.oqpt`の傾きを除きます。

```{r}
system.time(m_n3[[5]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_c+(1+Itemtype_c||subject)+(1|itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
モデルが収束しました。

```{r}
summary(m_n3[[5]])
```

相関パラメータを戻してみましょう。

```{r}
system.time(m_n3[[6]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_c+(1+Itemtype_c|subject)+(1|itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```

```{r}
anova(m_n3[[5]],m_n3[[6]])
```

Mtuschek et al. (2017)では，α ~LRT~ = .10 or .20の基準で有意ならより複雑なモデルを選択することが推奨されているので，相関パラメータがあるモデルを最終モデルとして採用しましょう。

## 最終モデル

```{r}
summary(m_n3[[6]])
```

## モデルの評価

VIFを確認しておきます。

```{r}
vif(m_n3[[6]])
```

すべて1程度ですので問題なさそうです。

## 解釈
結果の表を見てみると，固定効果の推定結果の一番下の行にある交互作用項が有意になっています。

## 交互作用項の図示
```{r}
plot_model(m_n3[[6]],type = "int")
```

図を見てみると，赤い線（Baseline）と青い線（Jonly）で傾きが違いそうです。では，実際にどうなのか見てみましょう。

## 単純主効果

Baseline項目をreference levelにしたダミー変数を作ります。

```{r}
ifelse(dat_n2$Itemtype == "Baseline", 0, 1) -> dat_n2$Itemtype_d_Base #Baselineをゼロにして基準レベルにします。
```

新しくリストを作ります。

```{r}
m_n4<-list()
```

さきほどのダミー変数を使ってもう一度最終モデルを分析します。
```{r}
system.time(m_n4[[1]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_d_Base+(1+Itemtype_d_Base|subject)+(1|itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
```{r}
summary(m_n4[[1]])
```

さきほどのモデルと比較して，`z.oqpt`の推定値が変わっていることがわかります。今，`Itemtype_d_Base`はBaseline項目がゼロのときですから，Baseline項目については`z.oqpt`の推定値が`r round(fixef(m_n4[[1]])[3],3)`だとわかります。

それでは，Jonly条件のときはどうでしょうか。今度はJonlyを0（reference level）にしたダミー変数を使って回帰モデルを作ってみます。

```{r}
ifelse(dat_n2$Itemtype == "Jonly", 0, 1) -> dat_n2$Itemtype_d_J #Jonlyをゼロにして基準レベルにします。
```

```{r}
system.time(m_n4[[2]] <-glmer(res ~ z.ColFreq_log+z.oqpt*Itemtype_d_J+(1+Itemtype_d_J|subject)+(1|itemID),
             data = dat_n2,
             family=binomial,
             glmerControl(optimizer=c("bobyqa"),optCtrl = list(maxfun=2e5))))
```
結果はいかに。

```{r}
summary(m_n4[[2]])
```

先ほどと比較すると，`z.oqpt`の推定値の符号が逆になっていることがわかります。しかしながら，傾きは有意ではありません。よって，Jonly条件のときも熟達度によって容認性判断課題の正答率が変わるとは言えないようです。

では交互作用が意味するところはなんなのでしょうか。可能性として，JonlyとBaselineの正答率の差が，熟達度によって変わるということが考えられます。これを調べるために，熟達度テストスコアのある特定の点におけるJonlyとBaselineの差を`emmeans()`関数を使って調べてみましょう。

```{r}
emmeans(m_n4[[2]],~z.oqpt*Itemtype_d_J,at=list(Itemtype_d_J=c("Jonly","Baseline"),z.oqpt=c(min(oqpt_sum$z.oqpt):max(oqpt_sum$z.oqpt))))%>% #範囲は熟達度テストのzスコアの最小値から最大値まで
contrast(.,"pairwise",by="z.oqpt")
```

### 解釈
- z.oqpt = -1.864のとき（熟達度が最も低いとき），2水準の差は-0.641で非有意になっています（_p_ = .067）。
- しかしながら，熟達度が高くなれば，JonlyとBaselineの差は統計的に有意になっています。
- この推定値は，Baselineを0としたダミー変数を使ったモデルでは係数の符号が逆になりますが，値は同じです（下記参照）。

```{r}
emmeans(m_n4[[1]],~z.oqpt*Itemtype_d_Base,at=list(Itemtype_d_Base=c("Baseline","Jonly"),z.oqpt=c(min(oqpt_sum$z.oqpt):max(oqpt_sum$z.oqpt))))%>% #範囲は熟達度テストのzスコアの最小値から最大値まで
contrast(.,"pairwise",by="z.oqpt")
```

zスコア化したOQPTスコアは素点ではどのくらいでしょうか？

```{r}
oqpt_sum %>% 
  arrange(oqpt) %>% 
  head
  
```

見てみると，26点です。これはCEFRでいうとA2レベルくらいです。A2レベルくらいだと，Baselineに対して「これは英語として自然ではない」と考えるのとJonlyに対して「これは英語として自然ではない」と答えるのが同じくらいの確率だということになります。

ただし，26点の学習者はたった1名であり，_p_値も有意水準の5%に近いので，この結果を強く主張することには慎重になるべきでしょう。

# 結果の報告
- 分析の結果を報告する際には，下記を報告しましょう。
  - 推定値（Estimate）
  - 標準誤差（_SE_）
  - 統計量（_z_値または_t_値）
  - _p_値
  - 変量効果の標準偏差
  - データポイント数
  - モデル式
  - 推定値の95%信頼区間
  -（R二乗）

## 95%信頼区間の計算
- 95%信頼区間の計算の最も簡単な方法は，`confint()`関数を使うことです
- lme4パッケージがロードされていたら，`confint()`とすると`confint.merMod()`が使われます
- 変量効果の信頼区間も計算すると，かなり時間がかかります
- 1つの固定効果の推定値の信頼区間を求めるだけでも，データセットの大きさによっては数時間以上かかることもあります（推定方法によって早くできるものもありますが，基本的には早い＝不正確）

# おわりに
- ロジスティック回帰については，下記のテクニカルレポートと，それに基づくWS資料もあります。合わせてお読みいただくと理解が深まるかと思います
  - 田村祐（2016）「外国語教育研究における二値データの分析－ロジスティック回帰を例に－」『外国語教育メディア学会中部支部外国語教育基礎研究部会2015年度報告論集』29–82.[リンク](https://www.letchubu.net/modules/xpwiki/?2015%E5%B9%B4%E5%BA%A6%E5%A0%B1%E5%91%8A%E8%AB%96%E9%9B%86)
  - 田村祐 (2019). 「統計ワークショップ」JACET英語語彙・英語辞書・リーディング研究会合同研究会. 早稲田大学.[資料](https://github.com/tam07pb915/JACET-SIG_GLMM-Workshop)
- 今回は割りとデータ分析のかなり泥臭い部分満載でしたが，これを補う資料が下記の資料です
- [一般化線形混合モデルの実践：気をつけたい3つのポイント](https://speakerdeck.com/tam07pb915/2021-11-06-lmm-and-glmm)
- 論文での報告など，今回扱いきれなかった部分は上記資料を御覧ください


# 参考文献リスト
- Barr, D. J., Levy, R., Scheepers, C., and Tily, H. J. (2013). Random effects structure for confirmatory hypothesis testing: Keep it maximal. _Journal of memory and language_, 68(3), 255–278.
- Bates, D., Kliegl, R., Vasishth, S., & Baayen, H. (2015). Parsimonious Mixed Models. https://arxiv.org/abs/1506.04967v2
- Brauer, M., & Curtin, J. J. (2018). Linear mixed-effects models and the analysis of nonindependent data: A unified framework to analyze categorical and continuous independent variables that vary within-subjects and/or within-items. _Psychological Methods_, 23(3), 389–411. https://doi.org/10.1037/met0000159
- Brysbaert, M., & Stevens, M. (2018). Power Analysis and Effect Size in Mixed Effects Models: A Tutorial. _Journal of Cognition_, 1(1), 9. https://doi.org/10.5334/joc.10
- Burnham, K. P., & Anderson, D. R. (2004). Multimodel Inference: Understanding AIC and BIC in Model Selection. _Sociological Methods & Research_, 33(2), 261–304. https://doi.org/10.1177/0049124104268644
- Frossard, J., & Renaud, O. (2019). Choosing the correlation structure of mixed effect models for experiments with stimuli. https://arxiv.org/abs/1903.10766v3
- Gries, S. T. (2021). (Generalized Linear) Mixed-Effects Modeling: A Learner Corpus Example. _Language Learning_, 71(3), 757–798. https://doi.org/10.1111/lang.12448
- Matuschek, H., Kliegl, R., Vasishth, S., Baayen, H., & Bates, D. (2017). Balancing Type I error and power in linear mixed models. _Journal of Memory and Language_, 94, 305–315. https://doi.org/10.1016/j.jml.2017.01.001
- Meteyard, L., & Davies, R. A. I. (2020). Best practice guidance for linear mixed-effects models in psychological science. _Journal of Memory and Language_, 112, 104092. https://doi.org/10.1016/j.jml.2020.104092
- Murakami, A. (2016). Modeling Systematicity and Individuality in Nonlinear Second Language Development: The Case of English Grammatical Morphemes: Modeling Individual Nonlinear Development. _Language Learning_, 66(4), 834–871. https://doi.org/10.1111/lang.12166
- RPubs—Reduction of Complexity of Linear Mixed Models with Double-Bar Syntax. (n.d.). Retrieved November 3, 2021, from https://rpubs.com/Reinhold/22193
- RPubs—The Correlation Parameter in the Random Effects of Mixed Effects Models. (n.d.). Retrieved November 3, 2021, from https://rpubs.com/yjunechoe/correlationsLMEM
- Schad, D. J., Vasishth, S., Hohenstein, S., & Kliegl, R. (2020). How to capitalize on a priori contrasts in linear (mixed) models: A tutorial. _Journal of Memory and Language_, 110, 104038. https://doi.org/10.1016/j.jml.2019.104038
- Scherbaum, C. A., & Ferreter, J. M. (2009). Estimating Statistical Power and Required Sample Sizes for Organizational Research Using Multilevel Modeling. _Organizational Research Methods_, 12(2), 347–367. https://doi.org/10.1177/1094428107308906
- Should we fit maximal linear mixed models? | R-bloggers. (2014, November 25). https://www.r-bloggers.com/2014/11/should-we-fit-maximal-linear-mixed-models/
- Tamura, Y., Fukuta, J., Nishimura, Y., Harada, Y., Hara, K., & Kato, D. (2019). Japanese EFL learners’ sentence processing of conceptual plurality: An analysis focusing on reciprocal verbs. _Applied Psycholinguistics_,41, 59–91. https://doi.org/10.1017/S0142716418000450
- 新井学, & Roland D. (2016). 「言語理解研究における眼球運動データ及び読み時間データの統計分析」. 統計数理, 64(2), 201–231.
- 井関龍太. (2020). 「心理学者は反応時間をどう分析するか」. 基礎心理学研究, 38(2), 243–249.
- 久保拓弥 (2012). 『データ解析のための統計モデリング入門』 東京: 岩波書店.
- 清水裕士 (2014).『個人と集団のマルチレベル分析』京都：ナカニシヤ出版.
-  田中豊・森川敏彦・山中竹春・ 冨田誠 (2008) 『一般化線形モデル入門 原著第2版』 共立出版.
- 田村祐（2016）「外国語教育研究における二値データの分析－ロジスティック回帰を例に－」『外国語教育メディア学会中部支部外国語教育基礎研究部会2015年度報告論集』29–82.[リンク](https://www.letchubu.net/modules/xpwiki/?2015%E5%B9%B4%E5%BA%A6%E5%A0%B1%E5%91%8A%E8%AB%96%E9%9B%86)
- 山口剛. (2018). 【心理学系】混合効果モデルの利用: 刺激の変量効果への対処 (2). 日本バイオフィードバック学会. https://doi.org/10.20595/jjbf.45.2_93

